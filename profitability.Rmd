---
title: "Predicting volume sales for new stock items and calculating their profitability"
author: "David Gibert Bosque"
date: "February, 2019"
output:
  rmdformats::readthedown:
    highlight: tango
---
```{r include=FALSE}
rm(list = ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
pacman::p_load(dplyr, corrplot, PerformanceAnalytics, rpart, rpart.plot)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
## Loading datasets & transforming to .Rdata for efficiency reasons ##

setwd("C:/Projects/Profitability/datasets/")

# existing = read.csv("existing.csv", sep = ",")
# new = read.csv("new.csv", sep = ",")

# save(existing, file = "existing.Rdata")
# save(new, file = "new.Rdata")

load(file = "existing.Rdata")
load(file = "new.Rdata")
```

## Summary of the project
Exploratory and Machine Learning process in order to predict the volume sales of new products so we can calculate the profitability of those new items introduced to the company portfolio.

### Looking at the structure of the data
```{r}
glimpse(existing)
```
**List of "TO DO" things to start with: **

1) *Adapting attribute names to something more comfortable*
2) *ProductID to factor & cheking it is unique for each observation*
3) *Best.Seller.Rank to factor and studying NA values*
4) *Depth, Width and Height to create Surface*
5) **LATER: Applying the same transformations to dataset NEW**

```{r}
names(existing) = c("type", "id", "price", "stars_5", "stars_4", "stars_3", "stars_2", "stars_1", "pos_service", "neg_service", "would_recommend", "bestSeller_rank", "weight", "depth", "width", "height", "profit_margin", "volume")

existing$id = factor(existing$id)
existing$bestSeller_rank = factor(existing$bestSeller_rank)

existing$surface = (existing$depth^2)+(existing$height^2)+(existing$width^2)

glimpse(existing)
```

## Looking at the summary of the attributes
```{r}
summary(existing)
```

### Do all the products have unique ID's?
```{r}
if (length(unique(existing$id)) == nrow(existing)) {
  print(paste("There are", length(unique(existing$id)), "unique ID values out of", nrow(existing), "observations."))
  } else { (print("CHECK: There is at least 1 repeated ID value in the 80 observations."))
}
```

### Is the best sellers rank field meaningful?
```{r}
barplot(table(existing$bestSeller_rank), main = "Amount of products\nby rank", col = "purple", xlab = "Rank", ylab = "Number of products")

sum(is.na(existing$bestSeller_rank))
```
*Rank value is not unique for each product. Attribute meaning is not clear. Later on, we will decide if the attribute is worth keeping or not, by applying a matrix correlation and a decision tree. Also, there are 15 NA values.*

## Price, Service review and User Recommendation histograms
```{r}
par(mfrow = c(2,2))

hist(existing$price, col = "purple", xlab = "Price €", main = "Histogram of Price")
hist(existing$would_recommend, col = "purple", xlab = "Would recommend?\n0 No, 1 Yes", main = "Histogram of user recommend")
hist(existing$pos_service, col = "purple", xlab = "Positive review", main = "Histogram of positive service")
hist(existing$neg_service, col = "purple", xlab = "Negative review", main = "Histogram of negative service")

par(mfrow = c(1,1))
```

* *Most of the products cost up to 500 €, there are some products which cost more than 1000 €*
* *Most of the products are recommended by the users*
* *Overall, there are more positive reviews for service than negative ones*

## Studying the dependent variable: VOLUME
```{r}
boxplot(existing$volume, col = "purple", varwidth = F)
```

*We can see there are a couple of outliers. Let's check which type of items are they.*

```{r}
existing %>%
  filter(volume >= 2500) %>%
  select(type, volume) %>%
  knitr::kable()
```

*An accesory and a game console are selling many units. We will not remove these outliers right now. When doing the machine learning process to predict Volume, the error metrics using the outliers and removing them will be checked in order to see what works better.*

## Correlation matrices
### Correlation matrix ALL attributes
```{r}
mat = as.matrix(existing[, -c(1:2, 12)])
corr_mat = cor(mat)
corrplot(corr = corr_mat, diag = F)
```

*The attributes that have bigger correlation with Volume are Star Reviews and positive service. Keep in mind that correlation here is linear, any quadratic correlation will not be shown in this matrix.*

*Also, it looks like there is collinearity between Star reviews attributes. Further investigation and decisions will avoid bias on our model later on.*

### Correlation matrix Star reviews and Volume
#### Studying correlation and collinearity
```{r}
mat_stars = as.matrix(existing[,c(4:8, 18)])
star_corr_mat = cor(mat_stars)
corrplot(corr = star_corr_mat, diag = F, method = "number")
```

*Volume and 5 star have a too-high correlation index. Basically, a correlation of 1 means that knowing the amount of 5 stars reviews of an item, we now exactly its sales volume, which does not make any sense in the real world. It is true that the more positive reviews an item has, the more it will sell, but not to the point where the amount of sales for any item is determined 100% on its 5 star reviews. In short words, there is something funny on the 5 star reviews values and we cannot use this data for later predictions.*

#### For tat reason, 5 star reviews attribute will be removed completely, and some feature engineering will be carried on the other star reviews attributes, aswell as in other fields, like we did creating the Surface attribute using the Depth, Height and Width.

#### Feature engineering will allow us to avoid collinearity among attributes and improve correlation and machine learning processes.

## Feature engineering
#### Let's create a bunch of attributes and see if they have better a correlation with "volume"
```{r}
existing = existing[, -c(2, 4, 13:16)] #Removing attributes that are not useful

existing$reviews_43 = existing$stars_4 + existing$stars_3
existing$reviews_21 = existing$stars_2 + existing$stars_1
existing$reviews_4321 = existing$stars_4 + existing$stars_3 + existing$stars_2 + existing$stars_1
existing$total_service_reviews = existing$pos_service + existing$neg_service
existing$total_positive_reviews = existing$stars_4 + existing$stars_3 + existing$pos_service
existing$total_negative_reviews = existing$stars_2 + existing$stars_1 + existing$neg_service

names(existing)
```


* *reviews_43* = Sum of `4 star & 3 star` attributes
* *reviews_21* = Sum of `2 star & 1 star` attributes
* *reviews_4321* = Sum of `4, 3, 2 & 1 star` attributes
* *total_service_reviews* = Sum of `positive` & `negative service` attributes
* *total_positive_reviews* = Sum of `4, 3 star` reviews and `positive service`
* *total_negative_reviews* = Sum of `2, 1 star` reviews and `negative service`

## Correlation matrix with newly created attributes
```{r}
mat_stars = as.matrix(existing[, -c(1, 10)])
star_corr_mat = cor(mat_stars)
corrplot(corr = star_corr_mat, diag = F, method = "number", number.cex = 0.5)
```

**Let's study this correlation matrix:**

*There are a lot of attributes now, correlated somehow with `volume`. We need to filter which attributes will be the best ones to work with for `volume` prediction.*

*Higher correlated attributes with `volume` are `4 star`, `3 star`, `positive service`, `4+3 star`, `total_service` and `total_positive`. What happens here is that we have a collinearity issue we have to fix here.*

*`4 star` is very correlated with `volume`, but also with `3 star`, `4+3 star` & `total positive reviews`. This means that these variables "explain" the same information. We do not want to keep all of them to avoid biasing our models later. We want to keep as much of that information as possible, so, we will keep the attribute that is more corretaled with `volume` that has no high correlation with other attributes at the same time. This means that, in this case, we will stick with `4 star` and will not use `3 star`, `4+3 star` & `total positive review`.*

*Must be said, though, that a correlation matrix only shows linear correlation between attributes, meaning that if there is some kind of quadratic correlation between `volume` and other attribute, it will not appear in the matrix correlation (correlation will be 0 or close to 0). In order to study other kinds of correlation rather than linear, we can use a Random Forest.*

## Let's build a Random Forest in order to see how the attributes distribute the information for the variable `volume`.
#### Attributes used are `price` + `stars_4` + `pos_service` + `neg_service` + `would_recommend` + `reviews_21`
```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
RF_all = rpart(formula = volume~., data = existing)
summary_RF_all = summary(RF_all)

RF_some = rpart(formula = volume~ price + stars_4 + pos_service + neg_service + would_recommend + reviews_21, data = existing)
summary_RF_some = summary(RF_some)
```

```{r}
summary_RF_some$variable.importance

rpart.plot(RF_some, type = 5, shadow.col = "black")
```

*We can see that the variable importance for the RF plot above defines `4 star`, `2+1 reviews`, `neg_service` & `pos_service` as the most important ones. This only means that each of these attributes appears many times in the Random Forest (they each appear many times in each tree in the random forest).*

**What really tells about importance is the plot, that shows which attributes are the most important ones for splitting the data, and those are `4 star` and `positive service`.**

#### Let's compare this random forest (built with some attributes) with a random forest using all the original attributes plus created ones at feature engineering section.
```{r}
summary_RF_all$variable.importance

rpart.plot(RF_all, type = 5, shadow.col = "black")
```

*We can see that the variable importance defines the same atributes than the last Random Forest. Also, the plot shows the same attributes for splitting the data.*

**Based on the correlation matrix and Random Forest approach, our final decision on which attributes are to be selected for predicting `volume` are `4 star` and `positive service`.**
```{r}
existing_2attributes = existing %>%
  select(stars_4, pos_service, volume)
```


## Machine Learning process. Predicting `Volume` sales.
#### We will be using LM, RF, KNN and SVM, then deciding which one works better and analysing the error later on.

#### Linear Regression
```{r}

```

#### Random Forest
```{r}

```

#### K-nearest neighbours
```{r}

```

#### Support Vector Machine
```{r}

```


```{r}
new_2attributes = new %>%
  select(X.4.Star.Reviews., X.Positive.Service.Review.) %>%
  transmute(stars_4 = as.integer(X.4.Star.Reviews.),
            pos_service = as.integer(X.Positive.Service.Review.))
```
